{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016319,
     "end_time": "2020-09-14T10:32:24.686621",
     "exception": false,
     "start_time": "2020-09-14T10:32:24.670302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 8.250954,
     "end_time": "2020-09-14T10:32:32.953612",
     "exception": false,
     "start_time": "2020-09-14T10:32:24.702658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.wcdjnk7yvmpzq2me2zzhjjrj3jikndb7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate, Average, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data) \n",
    "\n",
    "## このデータで何を予測するのか\n",
    "- コメントが有毒なコメントである確率を予測する。有毒なコメントは1.0。良性で毒性のないコメントは0.0。\n",
    "- テストセットでは、すべてのコメントが1.0または、0.0のいずれかに分類されます。\n",
    "\n",
    "## ファイル\n",
    "- jigsaw-toxic-comment-train.csv-最初のコンテストのデータ。データセットは、ウィキペディアのトークページ編集からの英語のコメントで構成されています。\n",
    "- jigsaw-unintended-bias-train.csv - 2番目のコンテストのデータ。これは、Civil Commentsデータセットの拡張バージョンであり、さまざまな追加ラベルが付いています。\n",
    "- sample_submission.csv-正しい形式のサンプル送信ファイル\n",
    "- test.csv-英語以外のさまざまな言語のウィキペディアトークページからのコメント。\n",
    "- test_labels.csv-テストデータのグラウンドトゥルースラベル（競争デッドリン後に追加されたデータ）\n",
    "- validate.csv-英語以外のさまざまな言語のウィキペディアトークページからのコメント。\n",
    "- jigsaw-toxic-comment-train-processed-seqlen128.csv -BERT用に前処理されたトレーニングデータ\n",
    "- jigsaw-unintended-bias-train-processed-seqlen128.csv -BERT用に前処理されたトレーニングデータ\n",
    "- 検証-処理済み-seqlen128.csv -BERT用に前処理された検証データ\n",
    "- test-processed-seqlen128.csv -BERT用に前処理されたテストデータ\n",
    "## 列\n",
    "- id-各ファイル内の識別子。\n",
    "- comment_text-分類されるコメントのテキスト。\n",
    "- lang-コメントの言語。\n",
    "- toxic-コメントがに分類されるかどうかtoxic。（には存在しませんtest.csv。）\n",
    "### Train1\n",
    "- severe_toxic-劇毒 \t\n",
    "- obscene-不適切\n",
    "- threat-脅迫\n",
    "- insult-侮辱\n",
    "- identity_hate-アイデンティティーヘイト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"D:\\JupyterNotebook\\jigsaw-multilingaual_Datasets\\jigsaw-toxic-comment-train.csv\")\n",
    "\n",
    "valid = pd.read_csv('D:\\JupyterNotebook\\jigsaw-multilingaual_Datasets\\\\validation.csv')\n",
    "\n",
    "test = pd.read_csv('D:\\JupyterNotebook\\jigsaw-multilingaual_Datasets\\\\test.csv')\n",
    "\n",
    "sub = pd.read_csv('D:\\JupyterNotebook\\jigsaw-multilingaual_Datasets\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016745,
     "end_time": "2020-09-14T10:32:32.989209",
     "exception": false,
     "start_time": "2020-09-14T10:32:32.972464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    def cleaning_text(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'[0-9\"]', '', text) # number\n",
    "        text = re.sub(r'#[\\S]+\\b', '', text) # hash\n",
    "        text = re.sub(r'@[\\S]+\\b', '', text) # mention\n",
    "        text = re.sub(r'https?\\S+', '', text) # link\n",
    "        text = re.sub(r'\\s+', ' ', text) # multiple white spaces\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def sub_text(text):\n",
    "        template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links \n",
    "        text = template.sub(r'', text)\n",
    "        \n",
    "        text = BeautifulSoup(text, 'lxml').get_text() #Removes HTML tags\n",
    "    \n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\" \"]+\")\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n",
    "        text = re.sub(' +', ' ', text) #Remove Extra Spaces\n",
    "        text = text.strip() # remove spaces at the beginning and at the end of string\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_split(text):\n",
    "        split_line = text.split(' ')\n",
    "        if(len(split_line) > 160):\n",
    "            text = ' '.join(split_line[:160]) + ' ' + ' '.join(split_line[-32:])\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    process_text = sub_text(text)\n",
    "    process_text = text_split(process_text)\n",
    "    \n",
    "    return process_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.026944,
     "end_time": "2020-09-14T10:32:33.120544",
     "exception": false,
     "start_time": "2020-09-14T10:32:33.0936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tk_encode(texts, tokenizer, maxlen=512):\n",
    "    \"\"\"\n",
    "    return_tensors : {\"PyTorch\" : 'pt'}, {\"TensorFlow\": 'tf'}, {\"NumPy\": 'np'} \n",
    "    \"\"\"\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_mask=True, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen,\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": enc_di['input_ids'],\n",
    "        \"attention_mask\": enc_di['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019688,
     "end_time": "2020-09-14T10:32:35.727854",
     "exception": false,
     "start_time": "2020-09-14T10:32:35.708166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TPU configs\n",
    "- TPUの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 5.350186,
     "end_time": "2020-09-14T10:32:41.097859",
     "exception": false,
     "start_time": "2020-09-14T10:32:35.747673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.030241,
     "end_time": "2020-09-14T10:32:41.149549",
     "exception": false,
     "start_time": "2020-09-14T10:32:41.119308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL = 'jplu/tf-xlm-roberta-large' # bert-base-uncased\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SEED = 42\n",
    "EPOCHS_1 = 20\n",
    "EPOCHS_2 = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "SHUFFLE = 2048\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020699,
     "end_time": "2020-09-14T10:32:41.19124",
     "exception": false,
     "start_time": "2020-09-14T10:32:41.170541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 変換実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.toxic = train.toxic.round().astype(int)\n",
    "train = train.sample(frac=1, random_state=SEED)\n",
    "valid = valid.sample(frac=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: text_preprocessing(x))\n",
    "valid['comment_text'] = valid['comment_text'].apply(lambda x: text_preprocessing(x))\n",
    "test['content'] = test['content'].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_memory: \u001b[33m12767760384\u001b[0m, memory_used: \u001b[33m7262695424\u001b[0m, memory_available, \u001b[33m5505064960\u001b[0m\n",
      "memory_used_per: 57%\n",
      "available_memory: 43%\n"
     ]
    }
   ],
   "source": [
    "import psutil \n",
    "from termcolor import colored\n",
    "\n",
    "# メモリ容量を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(f\"total_memory: {colored(mem.total, 'yellow')}, memory_used: {colored(mem.used, 'yellow')}, memory_available, {colored(mem.available, 'yellow')}\")\n",
    "print(f\"memory_used_per: {((mem.used / mem.total) * 100) :.0f}%\")\n",
    "print(f\"available_memory: {((mem.available / mem.total) * 100) :.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_steps = train.shape[0] // (BATCH_SIZE * 8)\n",
    "n_valid_steps = valid.shape[0] // (BATCH_SIZE)\n",
    "\n",
    "#del train1\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jplu/tf-xlm-roberta-large') # bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 324.393881,
     "end_time": "2020-09-14T10:39:25.086531",
     "exception": false,
     "start_time": "2020-09-14T10:34:00.69265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "x_train = tk_encode(list(train.comment_text.values), tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = tk_encode(list(valid.comment_text.values), tokenizer, maxlen=MAX_LEN)\n",
    "x_test = tk_encode(list(test.content.values), tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_valid = valid.toxic.values\n",
    "\n",
    "del train, valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021058,
     "end_time": "2020-09-14T10:39:25.128116",
     "exception": false,
     "start_time": "2020-09-14T10:39:25.107058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset objects\n",
    "- データセットがメモリに入る程に小さければ、データセットのcache()メソッドを使ってRAMにキャッシュする事で、時間が大幅に短縮される。\n",
    "- RAMへのキャッシングは、データをロード前処理したあと、シャッフル、リピート、バッチへの分割、プリフェッチ前に行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(SHUFFLE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n",
    "test1_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "del x_train, x_valid, y_train, y_valid, x_test, \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020471,
     "end_time": "2020-09-14T10:39:30.869121",
     "exception": false,
     "start_time": "2020-09-14T10:39:30.84865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValTrainRatioCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.sf}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "custom_call = ValTrainRatioCustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reducation = ReduceLROnPlateau(monitor=\"val_accuracy\", patience=2, mode=\"max\", verbose=0, factor=0.7, min_lr=1e-7)\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5, mode=\"max\", verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.03098,
     "end_time": "2020-09-14T10:39:30.920611",
     "exception": false,
     "start_time": "2020-09-14T10:39:30.889631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "callbacks_list = [learning_rate_reducation, early_stopping, custom_call]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020872,
     "end_time": "2020-09-14T10:39:30.962458",
     "exception": false,
     "start_time": "2020-09-14T10:39:30.941586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build model\n",
    "- クラス化する場合、内部の追跡やモデルの保存当で問題が発生する可能性があるので、どうしてもこの柔軟性が必要でない限りシーケンシャルAPIや関数型APIを使用した方が良い。\n",
    "- 以下では関数型APIを使用して各層の接続を行っている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 0.034992,
     "end_time": "2020-09-14T10:39:31.018124",
     "exception": false,
     "start_time": "2020-09-14T10:39:30.983132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    sequence_output = transformer({\"input_ids\": input_word_ids, \"attention_mask\": attention_mask})[0]\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(sequence_output)\n",
    "    max_pool = GlobalMaxPooling1D()(sequence_output)\n",
    "    cls_token = Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    samples = []\n",
    "    for n in range(2):\n",
    "        sample = Dropout(0.4)(cls_token)\n",
    "        sample = Dense(1, activation='sigmoid', name=f'sample_{n}')(sample)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    out = Average(name='output')(samples)\n",
    "    model = Model(inputs={\n",
    "                \"input_ids\": input_word_ids,\n",
    "                \"attention_mask\": attention_mask\n",
    "                }, \n",
    "                outputs=out,\n",
    "                name='XLM-R')\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020645,
     "end_time": "2020-09-14T10:39:31.059685",
     "exception": false,
     "start_time": "2020-09-14T10:39:31.03904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load to TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 142.229783,
     "end_time": "2020-09-14T10:41:53.30972",
     "exception": false,
     "start_time": "2020-09-14T10:39:31.079937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bb4053d13e4ea58ad1d0cdd166dab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m         raise ValueError(\n\u001b[0;32m    449\u001b[0m             \u001b[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1575\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m                 \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[0;32m   1578\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1845\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1847\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2140\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2142\u001b[1;33m             \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[0;32m   1999\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Downloading\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m     )\n\u001b[1;32m-> 2001\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m                 if (\n\u001b[0;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Machine_Learning\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024162,
     "end_time": "2020-09-14T10:41:53.360801",
     "exception": false,
     "start_time": "2020-09-14T10:41:53.336639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2129.665644,
     "end_time": "2020-09-14T11:17:23.051354",
     "exception": false,
     "start_time": "2020-09-14T10:41:53.38571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_history_1 = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_train_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS_1,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=VERBOSE\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.510705,
     "end_time": "2020-09-14T11:19:24.617701",
     "exception": false,
     "start_time": "2020-09-14T11:19:23.106996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 142.492637,
     "end_time": "2020-09-14T11:21:48.597347",
     "exception": false,
     "start_time": "2020-09-14T11:19:26.10471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Blending\n",
    "multi_sub = model.predict(test1_dataset, verbose=1)\n",
    "\n",
    "sub['toxic'] = multi_sub*0.75\n",
    "\n",
    "#Post-processing\n",
    "sub.loc[test[\"lang\"] == \"es\", \"toxic\"] *= 1.11\n",
    "sub.loc[test[\"lang\"] == \"fr\", \"toxic\"] *= 1.06\n",
    "sub.loc[test[\"lang\"] == \"it\", \"toxic\"] *= 0.93\n",
    "sub.loc[test[\"lang\"] == \"pt\", \"toxic\"] *= 0.92\n",
    "sub.loc[test[\"lang\"] == \"tr\", \"toxic\"] *= 0.94\n",
    "\n",
    "# min-max normalize\n",
    "sub.toxic -= sub.toxic.min()\n",
    "sub.toxic /= (sub.toxic.max() - sub.toxic.min())\n",
    "sub.toxic.hist(bins=100, log=False, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.794411,
     "end_time": "2020-09-14T11:21:52.125678",
     "exception": false,
     "start_time": "2020-09-14T11:21:50.331267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.126567,
     "end_time": "2020-09-14T11:21:55.986587",
     "exception": false,
     "start_time": "2020-09-14T11:21:53.86002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_memory: \u001b[33m12767760384\u001b[0m, memory_used: \u001b[33m7800610816\u001b[0m, memory_available, \u001b[33m4967149568\u001b[0m\n",
      "memory_used_per: 61%\n",
      "available_memory: 39%\n"
     ]
    }
   ],
   "source": [
    "import psutil \n",
    "from termcolor import colored\n",
    "\n",
    "# メモリ容量を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(f\"total_memory: {colored(mem.total, 'yellow')}, memory_used: {colored(mem.used, 'yellow')}, memory_available, {colored(mem.available, 'yellow')}\")\n",
    "print(f\"memory_used_per: {((mem.used / mem.total) * 100) :.0f}%\")\n",
    "print(f\"available_memory: {((mem.available / mem.total) * 100) :.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
